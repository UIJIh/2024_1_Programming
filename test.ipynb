{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9719e18e-f102-40aa-91ea-88dbd32086a0",
   "metadata": {},
   "source": [
    "# data 저장할 csv 파일 직접 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "10b830f3-b529-4036-96bb-be78b14b8912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diary_data.csv saved!\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "csv_file_name = 'diary_data.csv'\n",
    "# sample_data = [\n",
    "#     {'datetime': '1111-11-11', 'texts': 'THIS IS A SAMPLE', 'sentiment': 'GUESS WHAT'}\n",
    "# ]\n",
    "\n",
    "try:\n",
    "    with open(csv_file_name, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=['datetime', 'texts', 'sentiment'])\n",
    "        writer.writeheader()  # header(col)\n",
    "        # for row in sample_data:\n",
    "        #     writer.writerow(row)  # data가 있다면 row에 채워넣기\n",
    "    print(f\"{csv_file_name} saved!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Failed to create the file: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b3b7c4-b52a-49ba-b8d4-c36872e3d0a9",
   "metadata": {},
   "source": [
    "# 사용할 함수들 정의 \n",
    "    - diary 파일 읽기 \n",
    "    - diary 파일 (새로) 저장\n",
    "    - user texts 날짜와 함께 다이어리 [] 에 저장하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "82138745-fb42-465a-afbc-117f39f4e73c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "What date is it today?\n",
      "Your input must follow this format exactly! >> 2024-04-05\n",
      ":  2024-03-03\n",
      "Tell me How your day was!\n",
      ":  I HAD NOTHING TO DO..\n",
      "If you want to stop updating your diary, then press 0\n",
      ":  0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diary successfully updated & saved!\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "def load_diary():\n",
    "    diaries = []\n",
    "    try:\n",
    "        with open(\"diary_data.csv\", mode='r', encoding='utf-8') as file:\n",
    "            reader = csv.DictReader(file)\n",
    "            for row in reader:\n",
    "                date = row['datetime']\n",
    "                texts = row['texts']\n",
    "                sentiment = row['sentiment']\n",
    "                diaries.append({'datetime': date, 'texts': texts, 'sentiment': sentiment})\n",
    "    except FileNotFoundError:\n",
    "        print(\"No diary file found.\")\n",
    "    return diaries\n",
    "\n",
    "def save_diary(diaries):\n",
    "    try:\n",
    "        with open(\"diary_data.csv\", mode='w', newline='', encoding='utf-8') as file:\n",
    "            fieldnames = ['datetime', 'texts', 'sentiment']\n",
    "            writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "            for diary in diaries:\n",
    "                writer.writerow(diary)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to save the file: {e}\")        \n",
    "\n",
    "def get_user_texts(diaries):\n",
    "    while True:\n",
    "        date = input(\"What date is it today?\\nYour input must follow this format exactly! >> 2024-04-05\\n: \")\n",
    "        texts = input(\"Tell me How your day was!\\n: \")\n",
    "        diaries.append({'datetime': date, 'texts': texts, 'sentiment': None})\n",
    "        decision = input(\"If you want to stop updating your diary, then press 0\\n: \")\n",
    "        if decision == '0':\n",
    "            return diaries\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    diary = load_diary()\n",
    "    updated_diary = get_user_texts(diary)\n",
    "    save_diary(updated_diary)\n",
    "    print(\"Diary successfully updated & saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8ee47941-9316-4ae3-b1c2-9116436816f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'datetime': '1111-11-11',\n",
       "  'texts': 'THIS IS A SAMPLE',\n",
       "  'sentimental': 'GUESS WHAT'},\n",
       " {'datetime': '2024-04-03',\n",
       "  'texts': 'I HAD NOTHING TO DO..',\n",
       "  'sentimental': ''},\n",
       " {'datetime': '2024-03-03',\n",
       "  'texts': 'I HAD NOTHING TO DO..',\n",
       "  'sentimental': None}]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_diary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1ff934d7-3f1c-45f2-844c-d96ee745e87a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THIS IS A SAMPLE \n",
      "\n",
      "I HAD NOTHING TO DO.. \n",
      "\n",
      "I HAD NOTHING TO DO.. \n",
      "\n",
      "I HAD NOTHING TO DO..\n"
     ]
    }
   ],
   "source": [
    "for one in updated_diary:\n",
    "    print(one['texts'],'\\n')\n",
    "    \n",
    "print(updated_diary[1]['texts'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c74d87f-b4be-4c46-9445-364e9e245b0f",
   "metadata": {},
   "source": [
    "# 감정 분석 어떤 모델 쓸지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9d6d2259-4aef-46e9-b7c6-ef25302fca71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.9808839559555054}]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "sentiment_analyzer = pipeline('sentiment-analysis')  \n",
    "def sentiment_analysis(text):\n",
    "    return sentiment_analyzer(text)\n",
    "analyze_emotions(\"My day was hectic but productive. Back-to-back meetings in the morning, a quick lunch at my desk, and then spent the afternoon working on a big project. Managed to squeeze in a workout before dinner. Now I'm winding down with a good book.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802314d4-068e-41ad-b40a-8bed87ab882e",
   "metadata": {},
   "source": [
    "## 단순히 transformers만 가지고는 힘들어 보인다.<br>\n",
    "https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment-latest?text=Covid+cases+are+increasing+fast%21+But+i+know+and+i+hope+it+is+getting+way+better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d15556-7a5e-4d71-8cc5-5b74f299301a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3cffad05-88fd-4aed-aa62-a26e77ced330",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Neuer Ordner\\envs\\python3.9.16\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0d2bbea52ca497e821dc470d9bbbf47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/929 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eafe19f787d948548a1bd2385aaa23f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfad093bccaf4140a760447232b2bb3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfa918f606e84b05be1989403973359b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85829bd1b8b8468f989eba96d25b0f3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/501M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) positive 0.9236\n",
      "2) neutral 0.071\n",
      "3) negative 0.0055\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer, AutoConfig\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "\n",
    "def preprocess(text):\n",
    "    new_text = []\n",
    "    for t in text.split(\" \"):\n",
    "        t = '@user' if t.startswith('@') and len(t) > 1 else t\n",
    "        t = 'http' if t.startswith('http') else t\n",
    "        new_text.append(t)\n",
    "    return \" \".join(new_text)\n",
    "MODEL = f\"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "config = AutoConfig.from_pretrained(MODEL)\n",
    "# PT\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "#model.save_pretrained(MODEL)\n",
    "text = \"My day was hectic but productive. Back-to-back meetings in the morning, a quick lunch at my desk, and then spent the afternoon working on a big project. Managed to squeeze in a workout before dinner. Now I'm winding down with a good book.\"\n",
    "text = preprocess(text)\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "output = model(**encoded_input)\n",
    "scores = output[0][0].detach().numpy()\n",
    "scores = softmax(scores)\n",
    "# # TF\n",
    "# model = TFAutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "# model.save_pretrained(MODEL)\n",
    "# text = \"Covid cases are increasing fast!\"\n",
    "# encoded_input = tokenizer(text, return_tensors='tf')\n",
    "# output = model(encoded_input)\n",
    "# scores = output[0][0].numpy()\n",
    "# scores = softmax(scores)\n",
    "# Print labels and scores\n",
    "ranking = np.argsort(scores)\n",
    "ranking = ranking[::-1]\n",
    "for i in range(scores.shape[0]):\n",
    "    l = config.id2label[ranking[i]]\n",
    "    s = scores[ranking[i]]\n",
    "    print(f\"{i+1}) {l} {np.round(float(s), 4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "747bf3a7-5cfb-4020-8fef-0b8463aa67c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) positive 0.94\n",
      "2) neutral 0.06\n",
      "3) negative 0.0\n"
     ]
    }
   ],
   "source": [
    "text = \"My day was busy but fulfilling. Got the kids ready for school, did some house chores, and ran a few errands. In the afternoon, I helped the kids with their homework and made dinner. Now I'm enjoying some quiet time with a cup of tea.\"\n",
    "text = preprocess(text)\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "output = model(**encoded_input)\n",
    "scores = output[0][0].detach().numpy()\n",
    "scores = softmax(scores)\n",
    "ranking = np.argsort(scores)\n",
    "ranking = ranking[::-1]\n",
    "for i in range(scores.shape[0]):\n",
    "    l = config.id2label[ranking[i]]\n",
    "    s = scores[ranking[i]]\n",
    "    print(f\"{i+1}) {l} {np.round(float(s), 2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "33f15458-451f-45e6-9ccf-11524d9e44a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) neutral 0.57\n",
      "2) negative 0.23\n",
      "3) positive 0.2\n"
     ]
    }
   ],
   "source": [
    "text = \"In the morning was busy but fulfilling. Got the kids ready for school, did some house chores, and ran a few errands. However in the afternoon, I helped the kids with their homework too much so that i don't feel now doing something at all.\"\n",
    "text = preprocess(text)\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "output = model(**encoded_input)\n",
    "scores = output[0][0].detach().numpy()\n",
    "scores = softmax(scores)\n",
    "ranking = np.argsort(scores)\n",
    "ranking = ranking[::-1]\n",
    "for i in range(scores.shape[0]):\n",
    "    l = config.id2label[ranking[i]]\n",
    "    s = scores[ranking[i]]\n",
    "    print(f\"{i+1}) {l} {np.round(float(s), 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02984440-3001-450a-bcdb-fff825bf3e6d",
   "metadata": {},
   "source": [
    "## 이거 좋다!\n",
    ": This is a RoBERTa-base model trained on ~124M tweets from January 2018 to December 2021, and finetuned for sentiment analysis with the TweetEval benchmark."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e342023d-f64e-485b-a688-b58984e0074f",
   "metadata": {},
   "source": [
    "# 감정분석 모델 & 위에서 구현 함수들 합치기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "91f5a396-82ea-497e-9144-1673bede58c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TFAutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer, AutoConfig\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "MODEL = f\"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "LOCAL_MODEL_PATH = \"twitter-roberta\"\n",
    "\n",
    "# local에 모델 저장해서 매번 허깅페이스에서 안불러오도록 함\n",
    "if not os.path.exists(LOCAL_MODEL_PATH):\n",
    "    os.makedirs(LOCAL_MODEL_PATH)\n",
    "    \n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "tokenizer.save_pretrained(LOCAL_MODEL_PATH)\n",
    "\n",
    "config = AutoConfig.from_pretrained(MODEL)\n",
    "config.save_pretrained(LOCAL_MODEL_PATH)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "model.save_pretrained(LOCAL_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e653acef-4a00-4df3-b457-171ff72fd882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "What date is it today?\n",
      "Your input must follow this format exactly! >> 2024-04-05\n",
      ":  2024-05-05\n",
      "Tell me How your day was!\n",
      ":  My day was busy but fulfilling. Got the kids ready for school, did some house chores, and ran a few errands. In the afternoon, I helped the kids with their homework and made dinner. Now I'm enjoying some quiet time with a cup of tea.\n",
      "If you want to stop updating your diary, then press 0\n",
      ":  1\n",
      "What date is it today?\n",
      "Your input must follow this format exactly! >> 2024-04-05\n",
      ":  2024-05-10\n",
      "Tell me How your day was!\n",
      ":  My day was busy. Got the kids ready for school, did some house chores, and ran a few errands. In the afternoon, I helped the kids with their homework and made dinner. Now finally I'm enjoying some quiet time with a cup of tea.\n",
      "If you want to stop updating your diary, then press 0\n",
      ":  202\n",
      "What date is it today?\n",
      "Your input must follow this format exactly! >> 2024-04-05\n",
      ":  2024-05-02\n",
      "Tell me How your day was!\n",
      ":  My day was busy but fulfilling. Got the kids ready for school, did some house chores, and ran a few errands. In the afternoon, I helped the kids with their homework and made dinner. Now I'm enjoying some quiet time with a cup of tea.\n",
      "If you want to stop updating your diary, then press 0\n",
      ":  0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diary successfully updated & saved!\n"
     ]
    }
   ],
   "source": [
    "import csv, os\n",
    "from transformers import AutoModelForSequenceClassification, TFAutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer, AutoConfig\n",
    "from scipy.special import softmax\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def preprocess(text):\n",
    "    new_text = []\n",
    "    for t in text.split(\" \"):\n",
    "        t = '@user' if t.startswith('@') and len(t) > 1 else t\n",
    "        t = 'http' if t.startswith('http') else t\n",
    "        new_text.append(t)\n",
    "    return \" \".join(new_text)\n",
    "\n",
    "def load_model():\n",
    "    LOCAL_MODEL_PATH = \"twitter-roberta\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(LOCAL_MODEL_PATH)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(LOCAL_MODEL_PATH)\n",
    "    config = AutoConfig.from_pretrained(LOCAL_MODEL_PATH)\n",
    "    return tokenizer, model, config\n",
    "\n",
    "def load_diary():\n",
    "    diaries = []\n",
    "    try:\n",
    "        with open(\"diary_data.csv\", mode='r', encoding='utf-8') as file:\n",
    "            reader = csv.DictReader(file)\n",
    "            for row in reader:\n",
    "                date = row['datetime'] if row['datetime'] else None\n",
    "                texts = row['texts'] if row['texts'] else None\n",
    "                sentiment = row['sentiment'] if row['sentiment'] else None\n",
    "                diaries.append({'datetime': date, 'texts': texts, 'sentiment': sentiment})\n",
    "    except FileNotFoundError:\n",
    "        print(\"No diary file found.\")\n",
    "    return diaries\n",
    "\n",
    "def save_diary(diaries):\n",
    "    try:\n",
    "        with open(\"diary_data.csv\", mode='w', newline='', encoding='utf-8') as file:\n",
    "            fieldnames = ['datetime', 'texts', 'sentiment']\n",
    "            writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "            for diary in diaries:\n",
    "                writer.writerow(diary)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to save the file: {e}\")        \n",
    "\n",
    "def sentiment_analysis(tokenizer, model, config, text): \n",
    "    encoded_input = tokenizer(text, return_tensors='pt')\n",
    "    output = model(**encoded_input)\n",
    "    scores = output[0][0].detach().numpy()\n",
    "    scores = softmax(scores)\n",
    "    ranking = np.argsort(scores)\n",
    "    ranking = ranking[::-1]\n",
    "    sentiment_scores = {config.id2label[ranking[i]]: np.round(float(scores[ranking[i]]), 2) for i in range(scores.shape[0])}\n",
    "    # for i in range(scores.shape[0]):\n",
    "    #     l = config.id2label[ranking[i]]\n",
    "    #     s = scores[ranking[i]]\n",
    "    #     print(f\"{i+1}) {l} {np.round(float(s), 4)}\")\n",
    "    return sentiment_scores\n",
    "\n",
    "def get_user_texts(diaries):\n",
    "    while True:\n",
    "        date = input(\"What date is it today?\\nYour input must follow this format exactly! >> 2024-04-05\\n: \")\n",
    "        texts = input(\"Tell me How your day was!\\n: \")\n",
    "        sentiment_scores = sentiment_analysis(tokenizer, model, config, texts)\n",
    "        diaries.append({'datetime': date, 'texts': texts, 'sentiment': sentiment_scores})\n",
    "        decision = input(\"If you want to stop updating your diary, then press 0\\n: \")\n",
    "        if decision == '0':\n",
    "            return diaries\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    diary = load_diary()\n",
    "    tokenizer, model, config = load_model()\n",
    "    updated_diary = get_user_texts(diary)\n",
    "    save_diary(updated_diary)\n",
    "    print(\"Diary successfully updated & saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484c005f-e2f5-439b-8450-e5cc3badd526",
   "metadata": {},
   "source": [
    "# 날짜 최신순 정렬 + 정렬해야돼서 csv를 []말고 pandas로 읽어오는 것으로 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b72b1e5-8d01-49b6-91db-491d91b584ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'datetime': '2024-05-05',\n",
       "  'texts': \"My day was busy but fulfilling. Got the kids ready for school, did some house chores, and ran a few errands. In the afternoon, I helped the kids with their homework and made dinner. Now I'm enjoying some quiet time with a cup of tea.\",\n",
       "  'sentiment': {'positive': 0.94, 'neutral': 0.06, 'negative': 0.0}},\n",
       " {'datetime': '2024-05-10',\n",
       "  'texts': \"My day was busy. Got the kids ready for school, did some house chores, and ran a few errands. In the afternoon, I helped the kids with their homework and made dinner. Now finally I'm enjoying some quiet time with a cup of tea.\",\n",
       "  'sentiment': {'positive': 0.8, 'neutral': 0.19, 'negative': 0.01}},\n",
       " {'datetime': '2024-05-02',\n",
       "  'texts': \"My day was busy but fulfilling. Got the kids ready for school, did some house chores, and ran a few errands. In the afternoon, I helped the kids with their homework and made dinner. Now I'm enjoying some quiet time with a cup of tea.\",\n",
       "  'sentiment': {'positive': 0.94, 'neutral': 0.06, 'negative': 0.0}}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_diary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "364c27c3-dbdf-47f9-858e-168a3b75a62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "diaries = pd.DataFrame(updated_diary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "25dc0819-67ec-4303-a88c-230bac1b296e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "diaries['datetime'] = pd.to_datetime(diaries['datetime'])\n",
    "diaries = diaries.sort_values(by='datetime', ascending=False) # 최신순 descending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "034c095f-aa9c-47e4-b7c4-24a9dd1c87c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>texts</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-05-10</td>\n",
       "      <td>My day was busy. Got the kids ready for school...</td>\n",
       "      <td>{'positive': 0.8, 'neutral': 0.19, 'negative':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-05-05</td>\n",
       "      <td>My day was busy but fulfilling. Got the kids r...</td>\n",
       "      <td>{'positive': 0.94, 'neutral': 0.06, 'negative'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-05-02</td>\n",
       "      <td>My day was busy but fulfilling. Got the kids r...</td>\n",
       "      <td>{'positive': 0.94, 'neutral': 0.06, 'negative'...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    datetime                                              texts  \\\n",
       "1 2024-05-10  My day was busy. Got the kids ready for school...   \n",
       "0 2024-05-05  My day was busy but fulfilling. Got the kids r...   \n",
       "2 2024-05-02  My day was busy but fulfilling. Got the kids r...   \n",
       "\n",
       "                                           sentiment  \n",
       "1  {'positive': 0.8, 'neutral': 0.19, 'negative':...  \n",
       "0  {'positive': 0.94, 'neutral': 0.06, 'negative'...  \n",
       "2  {'positive': 0.94, 'neutral': 0.06, 'negative'...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0fb949e8-350b-4a7e-80e8-ece862abb9d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datetime     datetime64[ns]\n",
      "texts                object\n",
      "sentiment            object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(diaries.dtypes) # datetime type이 datetime인 것을 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0b525702-e4dc-4bac-ae1f-231ea375d4e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "What date is it today?\n",
      "Your input must follow this format exactly! >> 2024-04-05\n",
      ":  2024-05-22\n",
      "Tell me How your day was!\n",
      ":  My day was hectic but productive. Back-to-back meetings in the morning, a quick lunch at my desk, and then spent the afternoon working on a big project. Managed to squeeze in a workout before dinner. Now I'm winding down with a good book.\n",
      "If you want to stop updating your diary, then press 0\n",
      ":  0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diary successfully updated & saved!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoConfig\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "\n",
    "def preprocess(text):\n",
    "    new_text = []\n",
    "    for t in text.split(\" \"):\n",
    "        t = '@user' if t.startswith('@') and len(t) > 1 else t\n",
    "        t = 'http' if t.startswith('http') else t\n",
    "        new_text.append(t)\n",
    "    return \" \".join(new_text)\n",
    "\n",
    "def load_model():\n",
    "    LOCAL_MODEL_PATH = \"twitter-roberta\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(LOCAL_MODEL_PATH)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(LOCAL_MODEL_PATH)\n",
    "    config = AutoConfig.from_pretrained(LOCAL_MODEL_PATH)\n",
    "    return tokenizer, model, config\n",
    "\n",
    "def load_diary():\n",
    "    try:\n",
    "        diaries = pd.read_csv(\"diary_data.csv\", parse_dates=['datetime'])\n",
    "    except FileNotFoundError: # 내용이 없어도 만들어야 함\n",
    "        diaries = pd.DataFrame(columns=['datetime', 'texts', 'sentiment'])\n",
    "    return diaries\n",
    "\n",
    "def save_diary(diaries):\n",
    "    try:\n",
    "        diaries.to_csv(\"diary_data.csv\", index=False, encoding='utf-8')\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to save the file: {e}\")    \n",
    "\n",
    "def sort_diary(diaries):\n",
    "    diaries['datetime'] = pd.to_datetime(diaries['datetime'], format=\"%Y-%m-%d\") # datetime 객체로 변환\n",
    "    diaries = diaries.sort_values(by='datetime', ascending=False) # 최신순, descending\n",
    "    return diaries\n",
    "\n",
    "def sentiment_analysis(tokenizer, model, config, text): \n",
    "    encoded_input = tokenizer(text, return_tensors='pt')\n",
    "    output = model(**encoded_input)\n",
    "    scores = output[0][0].detach().numpy()\n",
    "    scores = softmax(scores)\n",
    "    ranking = np.argsort(scores)\n",
    "    ranking = ranking[::-1]\n",
    "    sentiment_scores = {config.id2label[ranking[i]]: np.round(float(scores[ranking[i]]), 2) for i in range(scores.shape[0])}\n",
    "    return sentiment_scores\n",
    "\n",
    "def get_user_texts(diaries):\n",
    "    while True:\n",
    "        date = input(\"What date is it today?\\nYour input must follow this format exactly! >> 2024-04-05\\n: \")\n",
    "        texts = input(\"Tell me How your day was!\\n: \")\n",
    "        sentiment_scores = sentiment_analysis(tokenizer, model, config, texts)\n",
    "        new_diary = pd.DataFrame([{'datetime': date, 'texts': texts, 'sentiment': sentiment_scores}])\n",
    "        diaries = pd.concat([diaries, new_diary], ignore_index=True)\n",
    "        diaries = sort_diary(diaries)\n",
    "        decision = input(\"If you want to stop updating your diary, then press 0\\n: \")\n",
    "        if decision == '0':\n",
    "            return diaries\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    diary = load_diary()\n",
    "    tokenizer, model, config = load_model()\n",
    "    updated_diary = get_user_texts(diary)\n",
    "    save_diary(updated_diary)\n",
    "    print(\"Diary successfully updated & saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0eff792c-e59e-4721-bb12-b96273c3795c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>texts</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-05-22</td>\n",
       "      <td>My day was hectic but productive. Back-to-back...</td>\n",
       "      <td>{'positive': 0.92, 'neutral': 0.07, 'negative'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-05-06</td>\n",
       "      <td>Oh.. I don't really feel going to school today...</td>\n",
       "      <td>{'negative': 0.53, 'neutral': 0.28, 'positive'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-05-05</td>\n",
       "      <td>Yayy!! Children's day!! I have a lot to say ab...</td>\n",
       "      <td>{'positive': 0.98, 'neutral': 0.02, 'negative'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-05-02</td>\n",
       "      <td>My day was busy but fulfilling. Got the kids r...</td>\n",
       "      <td>{'positive': 0.94, 'neutral': 0.06, 'negative'...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    datetime                                              texts  \\\n",
       "3 2024-05-22  My day was hectic but productive. Back-to-back...   \n",
       "2 2024-05-06  Oh.. I don't really feel going to school today...   \n",
       "1 2024-05-05  Yayy!! Children's day!! I have a lot to say ab...   \n",
       "0 2024-05-02  My day was busy but fulfilling. Got the kids r...   \n",
       "\n",
       "                                           sentiment  \n",
       "3  {'positive': 0.92, 'neutral': 0.07, 'negative'...  \n",
       "2  {'negative': 0.53, 'neutral': 0.28, 'positive'...  \n",
       "1  {'positive': 0.98, 'neutral': 0.02, 'negative'...  \n",
       "0  {'positive': 0.94, 'neutral': 0.06, 'negative'...  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_diary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f96b29d0-eb5c-4e70-aef0-c0184399db27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3   2024-05-22\n",
      "2   2024-05-06\n",
      "1   2024-05-05\n",
      "0   2024-05-02\n",
      "Name: datetime, dtype: datetime64[ns]\n",
      "datetime64[ns]\n"
     ]
    }
   ],
   "source": [
    "print(updated_diary['datetime'].head())\n",
    "print(updated_diary['datetime'].dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d52a07-d766-48ba-b9d3-d8401ea07d2e",
   "metadata": {},
   "source": [
    "# text-to-image 생성 및 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0820beb7-db5b-4ed9-b4ce-0d4d305c7f3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Neuer Ordner\\envs\\python3.9.16\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6726723d5f74592a2f04a3e516b1da9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 19 files:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Neuer Ordner\\envs\\python3.9.16\\Lib\\site-packages\\huggingface_hub\\file_download.py:982: UserWarning: Not enough free disk space to download the file. The expected file size is: 5135.15 MB. The target location C:\\Users\\황의지\\.cache\\huggingface\\hub\\models--stabilityai--stable-diffusion-xl-base-1.0\\blobs only has 0.00 MB free disk space.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fc988e97cdf4e11bdd150d3569b6030",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "diffusion_pytorch_model.fp16.safetensors:  45%|####5     | 2.33G/5.14G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 28] No space left on device",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[51], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# load both base & refiner\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m base \u001b[38;5;241m=\u001b[39m \u001b[43mDiffusionPipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstabilityai/stable-diffusion-xl-base-1.0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat16\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfp16\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_safetensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[0;32m      7\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m base\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      9\u001b[0m refiner \u001b[38;5;241m=\u001b[39m DiffusionPipeline\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstabilityai/stable-diffusion-xl-refiner-1.0\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     11\u001b[0m     text_encoder_2\u001b[38;5;241m=\u001b[39mbase\u001b[38;5;241m.\u001b[39mtext_encoder_2,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     15\u001b[0m     variant\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfp16\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     16\u001b[0m )\n",
      "File \u001b[1;32mC:\\Neuer Ordner\\envs\\python3.9.16\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[0;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Neuer Ordner\\envs\\python3.9.16\\Lib\\site-packages\\diffusers\\pipelines\\pipeline_utils.py:671\u001b[0m, in \u001b[0;36mDiffusionPipeline.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    666\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pretrained_model_name_or_path\u001b[38;5;241m.\u001b[39mcount(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    667\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    668\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe provided pretrained_model_name_or_path \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    669\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is neither a valid local path nor a valid repo id. Please check the parameter.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    670\u001b[0m         )\n\u001b[1;32m--> 671\u001b[0m     cached_folder \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    679\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    680\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_flax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    681\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_safetensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_safetensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    682\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_onnx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_onnx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    683\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcustom_pipeline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_pipeline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    684\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcustom_revision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_revision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    685\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvariant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvariant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    686\u001b[0m \u001b[43m        \u001b[49m\u001b[43mload_connected_pipeline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mload_connected_pipeline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    687\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    688\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    689\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    690\u001b[0m     cached_folder \u001b[38;5;241m=\u001b[39m pretrained_model_name_or_path\n",
      "File \u001b[1;32mC:\\Neuer Ordner\\envs\\python3.9.16\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[0;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Neuer Ordner\\envs\\python3.9.16\\Lib\\site-packages\\diffusers\\pipelines\\pipeline_utils.py:1406\u001b[0m, in \u001b[0;36mDiffusionPipeline.download\u001b[1;34m(cls, pretrained_model_name, **kwargs)\u001b[0m\n\u001b[0;32m   1404\u001b[0m \u001b[38;5;66;03m# download all allow_patterns - ignore_patterns\u001b[39;00m\n\u001b[0;32m   1405\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1406\u001b[0m     cached_folder \u001b[38;5;241m=\u001b[39m \u001b[43msnapshot_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_patterns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_patterns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1415\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_patterns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_patterns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1416\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1417\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1419\u001b[0m     \u001b[38;5;66;03m# retrieve pipeline class from local file\u001b[39;00m\n\u001b[0;32m   1420\u001b[0m     cls_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mload_config(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(cached_folder, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_index.json\u001b[39m\u001b[38;5;124m\"\u001b[39m))\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_class_name\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32mC:\\Neuer Ordner\\envs\\python3.9.16\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[0;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Neuer Ordner\\envs\\python3.9.16\\Lib\\site-packages\\huggingface_hub\\_snapshot_download.py:294\u001b[0m, in \u001b[0;36msnapshot_download\u001b[1;34m(repo_id, repo_type, revision, cache_dir, local_dir, library_name, library_version, user_agent, proxies, etag_timeout, force_download, token, local_files_only, allow_patterns, ignore_patterns, max_workers, tqdm_class, headers, endpoint, local_dir_use_symlinks, resume_download)\u001b[0m\n\u001b[0;32m    292\u001b[0m         _inner_hf_hub_download(file)\n\u001b[0;32m    293\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 294\u001b[0m     \u001b[43mthread_map\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_inner_hf_hub_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfiltered_repo_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mFetching \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfiltered_repo_files\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m files\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    298\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_workers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    299\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# User can use its own tqdm class or the default one from `huggingface_hub.utils`\u001b[39;49;00m\n\u001b[0;32m    300\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtqdm_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtqdm_class\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mhf_tqdm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    301\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m local_dir \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    304\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mrealpath(local_dir))\n",
      "File \u001b[1;32mC:\\Neuer Ordner\\envs\\python3.9.16\\Lib\\site-packages\\tqdm\\contrib\\concurrent.py:69\u001b[0m, in \u001b[0;36mthread_map\u001b[1;34m(fn, *iterables, **tqdm_kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;124;03mEquivalent of `list(map(fn, *iterables))`\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;124;03mdriven by `concurrent.futures.ThreadPoolExecutor`.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;124;03m    [default: max(32, cpu_count() + 4)].\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mconcurrent\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfutures\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ThreadPoolExecutor\n\u001b[1;32m---> 69\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_executor_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43mThreadPoolExecutor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43miterables\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtqdm_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Neuer Ordner\\envs\\python3.9.16\\Lib\\site-packages\\tqdm\\contrib\\concurrent.py:51\u001b[0m, in \u001b[0;36m_executor_map\u001b[1;34m(PoolExecutor, fn, *iterables, **tqdm_kwargs)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ensure_lock(tqdm_class, lock_name\u001b[38;5;241m=\u001b[39mlock_name) \u001b[38;5;28;01mas\u001b[39;00m lk:\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;66;03m# share lock in case workers are already using `tqdm`\u001b[39;00m\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m PoolExecutor(max_workers\u001b[38;5;241m=\u001b[39mmax_workers, initializer\u001b[38;5;241m=\u001b[39mtqdm_class\u001b[38;5;241m.\u001b[39mset_lock,\n\u001b[0;32m     50\u001b[0m                       initargs\u001b[38;5;241m=\u001b[39m(lk,)) \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[1;32m---> 51\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtqdm_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43miterables\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Neuer Ordner\\envs\\python3.9.16\\Lib\\site-packages\\tqdm\\notebook.py:250\u001b[0m, in \u001b[0;36mtqdm_notebook.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    249\u001b[0m     it \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__iter__\u001b[39m()\n\u001b[1;32m--> 250\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mit\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    251\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# return super(tqdm...) will not catch exception\u001b[39;49;00m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[38;5;66;03m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Neuer Ordner\\envs\\python3.9.16\\Lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   1182\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[0;32m   1183\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[0;32m   1184\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "File \u001b[1;32mC:\\Neuer Ordner\\envs\\python3.9.16\\Lib\\concurrent\\futures\\_base.py:619\u001b[0m, in \u001b[0;36mExecutor.map.<locals>.result_iterator\u001b[1;34m()\u001b[0m\n\u001b[0;32m    616\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m fs:\n\u001b[0;32m    617\u001b[0m     \u001b[38;5;66;03m# Careful not to keep a reference to the popped future\u001b[39;00m\n\u001b[0;32m    618\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 619\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[43m_result_or_cancel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    620\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    621\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m _result_or_cancel(fs\u001b[38;5;241m.\u001b[39mpop(), end_time \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic())\n",
      "File \u001b[1;32mC:\\Neuer Ordner\\envs\\python3.9.16\\Lib\\concurrent\\futures\\_base.py:317\u001b[0m, in \u001b[0;36m_result_or_cancel\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    316\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 317\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfut\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    319\u001b[0m         fut\u001b[38;5;241m.\u001b[39mcancel()\n",
      "File \u001b[1;32mC:\\Neuer Ordner\\envs\\python3.9.16\\Lib\\concurrent\\futures\\_base.py:456\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    454\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    455\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m--> 456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    458\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "File \u001b[1;32mC:\\Neuer Ordner\\envs\\python3.9.16\\Lib\\concurrent\\futures\\_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[0;32m    400\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 401\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    403\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    404\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Neuer Ordner\\envs\\python3.9.16\\Lib\\concurrent\\futures\\thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "File \u001b[1;32mC:\\Neuer Ordner\\envs\\python3.9.16\\Lib\\site-packages\\huggingface_hub\\_snapshot_download.py:268\u001b[0m, in \u001b[0;36msnapshot_download.<locals>._inner_hf_hub_download\u001b[1;34m(repo_file)\u001b[0m\n\u001b[0;32m    267\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_inner_hf_hub_download\u001b[39m(repo_file: \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m--> 268\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    276\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_dir_use_symlinks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_dir_use_symlinks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    277\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlibrary_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlibrary_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    278\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlibrary_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlibrary_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    279\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    280\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    281\u001b[0m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    282\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    283\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    284\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    285\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    286\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Neuer Ordner\\envs\\python3.9.16\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[0;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Neuer Ordner\\envs\\python3.9.16\\Lib\\site-packages\\huggingface_hub\\file_download.py:1221\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[1;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, legacy_cache_layout, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[0;32m   1202\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _hf_hub_download_to_local_dir(\n\u001b[0;32m   1203\u001b[0m         \u001b[38;5;66;03m# Destination\u001b[39;00m\n\u001b[0;32m   1204\u001b[0m         local_dir\u001b[38;5;241m=\u001b[39mlocal_dir,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1218\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[0;32m   1219\u001b[0m     )\n\u001b[0;32m   1220\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1221\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1222\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[0;32m   1223\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1224\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[0;32m   1225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1229\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[0;32m   1230\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1231\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1232\u001b[0m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1233\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1234\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[0;32m   1235\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1236\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1237\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Neuer Ordner\\envs\\python3.9.16\\Lib\\site-packages\\huggingface_hub\\file_download.py:1367\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[1;34m(cache_dir, repo_id, filename, repo_type, revision, headers, proxies, etag_timeout, endpoint, local_files_only, force_download)\u001b[0m\n\u001b[0;32m   1365\u001b[0m Path(lock_path)\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39mmkdir(parents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   1366\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m WeakFileLock(lock_path):\n\u001b[1;32m-> 1367\u001b[0m     \u001b[43m_download_to_tmp_and_move\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1368\u001b[0m \u001b[43m        \u001b[49m\u001b[43mincomplete_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.incomplete\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1369\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdestination_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1370\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl_to_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1371\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1372\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1373\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1374\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1375\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1376\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1377\u001b[0m     _create_symlink(blob_path, pointer_path, new_blob\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   1379\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pointer_path\n",
      "File \u001b[1;32mC:\\Neuer Ordner\\envs\\python3.9.16\\Lib\\site-packages\\huggingface_hub\\file_download.py:1884\u001b[0m, in \u001b[0;36m_download_to_tmp_and_move\u001b[1;34m(incomplete_path, destination_path, url_to_download, proxies, headers, expected_size, filename, force_download)\u001b[0m\n\u001b[0;32m   1881\u001b[0m         _check_disk_space(expected_size, incomplete_path\u001b[38;5;241m.\u001b[39mparent)\n\u001b[0;32m   1882\u001b[0m         _check_disk_space(expected_size, destination_path\u001b[38;5;241m.\u001b[39mparent)\n\u001b[1;32m-> 1884\u001b[0m     \u001b[43mhttp_get\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m        \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1891\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1893\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownload complete. Moving file to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdestination_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1894\u001b[0m _chmod_and_move(incomplete_path, destination_path)\n",
      "File \u001b[1;32mC:\\Neuer Ordner\\envs\\python3.9.16\\Lib\\site-packages\\huggingface_hub\\file_download.py:542\u001b[0m, in \u001b[0;36mhttp_get\u001b[1;34m(url, temp_file, proxies, resume_size, headers, expected_size, displayed_filename, _nb_retries, _tqdm_bar)\u001b[0m\n\u001b[0;32m    540\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunk:  \u001b[38;5;66;03m# filter out keep-alive new chunks\u001b[39;00m\n\u001b[0;32m    541\u001b[0m     progress\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mlen\u001b[39m(chunk))\n\u001b[1;32m--> 542\u001b[0m     \u001b[43mtemp_file\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    543\u001b[0m     new_resume_size \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(chunk)\n\u001b[0;32m    544\u001b[0m     \u001b[38;5;66;03m# Some data has been downloaded from the server so we reset the number of retries.\u001b[39;00m\n",
      "\u001b[1;31mOSError\u001b[0m: [Errno 28] No space left on device"
     ]
    }
   ],
   "source": [
    "from diffusers import DiffusionPipeline\n",
    "import torch\n",
    "\n",
    "# load both base & refiner\n",
    "base = DiffusionPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16, variant=\"fp16\", use_safetensors=True\n",
    ")\n",
    "base.to(\"cuda\")\n",
    "refiner = DiffusionPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-refiner-1.0\",\n",
    "    text_encoder_2=base.text_encoder_2,\n",
    "    vae=base.vae,\n",
    "    torch_dtype=torch.float16,\n",
    "    use_safetensors=True,\n",
    "    variant=\"fp16\",\n",
    ")\n",
    "refiner.to(\"cuda\")\n",
    "\n",
    "# Define how many steps and what % of steps to be run on each experts (80/20) here\n",
    "n_steps = 40\n",
    "high_noise_frac = 0.8\n",
    "\n",
    "prompt = \"A majestic lion jumping from a big stone at night\"\n",
    "\n",
    "# run both experts\n",
    "image = base(\n",
    "    prompt=prompt,\n",
    "    num_inference_steps=n_steps,\n",
    "    denoising_end=high_noise_frac,\n",
    "    output_type=\"latent\",\n",
    ").images\n",
    "image = refiner(\n",
    "    prompt=prompt,\n",
    "    num_inference_steps=n_steps,\n",
    "    denoising_start=high_noise_frac,\n",
    "    image=image,\n",
    ").images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1115dda-3e6d-49ce-b33e-7928203f16df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import DiffusionPipeline\n",
    "import torch\n",
    "\n",
    "def save_models():\n",
    "    base_model = \"stabilityai/stable-diffusion-xl-base-1.0\"\n",
    "    refiner_model = \"stabilityai/stable-diffusion-xl-refiner-1.0\"\n",
    "    local_base_path = \"table_diffusion_xl_base\"\n",
    "    local_refiner_path = \"stable_diffusion_xl_refiner\"\n",
    "    \n",
    "    base = DiffusionPipeline.from_pretrained(\n",
    "        base_model, torch_dtype=torch.float16, variant=\"fp16\", use_safetensors=True\n",
    "    )\n",
    "    base.save_pretrained(local_base_path)\n",
    "    \n",
    "    refiner = DiffusionPipeline.from_pretrained(\n",
    "        refiner_model_name,\n",
    "        text_encoder_2=base.text_encoder_2,\n",
    "        vae=base.vae,\n",
    "        torch_dtype=torch.float16,\n",
    "        use_safetensors=True,\n",
    "        variant=\"fp16\",\n",
    "    )\n",
    "    refiner.save_pretrained(local_refiner_path)\n",
    "\n",
    "save_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d63d502-34d5-4360-befb-9ddce82b906a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [],
   "source": [
    "from diffusers import DiffusionPipeline\n",
    "import torch\n",
    "\n",
    "def load_models(prompt, n_steps=40, high_noise_frac=0.8):\n",
    "    local_base_path = \"stable_diffusion_xl_base\"\n",
    "    local_refiner_path = \"stable_diffusion_xl_refiner\"\n",
    "\n",
    "    # Load base model from local path\n",
    "    base = DiffusionPipeline.from_pretrained(\n",
    "        local_base_path, torch_dtype=torch.float16, variant=\"fp16\", use_safetensors=True\n",
    "    )\n",
    "    base.to(\"cuda\")\n",
    "\n",
    "    # Load refiner model from local path\n",
    "    refiner = DiffusionPipeline.from_pretrained(\n",
    "        local_refiner_path,\n",
    "        text_encoder_2=base.text_encoder_2,\n",
    "        vae=base.vae,\n",
    "        torch_dtype=torch.float16,\n",
    "        use_safetensors=True,\n",
    "        variant=\"fp16\",\n",
    "    )\n",
    "    refiner.to(\"cuda\")\n",
    "\n",
    "    # Run both models to generate the image\n",
    "    image = base(\n",
    "        prompt=prompt,\n",
    "        num_inference_steps=n_steps,\n",
    "        denoising_end=high_noise_frac,\n",
    "        output_type=\"latent\",\n",
    "    ).images\n",
    "\n",
    "    image = refiner(\n",
    "        prompt=prompt,\n",
    "        num_inference_steps=n_steps,\n",
    "        denoising_start=high_noise_frac,\n",
    "        image=image,\n",
    "    ).images[0]\n",
    "\n",
    "    return image\n",
    "\n",
    "# Example usage\n",
    "prompt = \"A majestic lion jumping from a big stone at night\"\n",
    "image = load_models(prompt)\n",
    "image.save(\"generated_image.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17a6d18-778c-4ce5-be43-c3b76dee89de",
   "metadata": {},
   "source": [
    "## 저장하기엔.. 컴퓨터 용량이 없다 (매번 GPU를 못쓰니까..)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a2a2f2-2568-4d32-9b25-ac445f068a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import DiffusionPipeline\n",
    "import torch\n",
    "\n",
    "# load both base & refiner\n",
    "base = DiffusionPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16, variant=\"fp16\", use_safetensors=True\n",
    ")\n",
    "base.to(\"cuda\")\n",
    "refiner = DiffusionPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-refiner-1.0\",\n",
    "    text_encoder_2=base.text_encoder_2,\n",
    "    vae=base.vae,\n",
    "    torch_dtype=torch.float16,\n",
    "    use_safetensors=True,\n",
    "    variant=\"fp16\",\n",
    ")\n",
    "refiner.to(\"cuda\")\n",
    "\n",
    "# Define how many steps and what % of steps to be run on each experts (80/20) here\n",
    "n_steps = 40\n",
    "high_noise_frac = 0.8\n",
    "\n",
    "prompt = \"Had a relaxing day. Went for a walk in the park in the morning, did some gardening, and read a few chapters of a novel. In the afternoon, I met friends for coffee and now I'm watching a movie at home.\"\n",
    "\n",
    "# run both experts\n",
    "image = base(\n",
    "    prompt=prompt,\n",
    "    num_inference_steps=n_steps,\n",
    "    denoising_end=high_noise_frac,\n",
    "    output_type=\"latent\",\n",
    ").images\n",
    "image = refiner(\n",
    "    prompt=prompt,\n",
    "    num_inference_steps=n_steps,\n",
    "    denoising_start=high_noise_frac,\n",
    "    image=image,\n",
    ").images[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ba5716-ed28-4c08-b5d2-b56c032bc662",
   "metadata": {},
   "source": [
    "1. **A Busy Professional**:\n",
    "   \"My day was hectic but productive. Back-to-back meetings in the morning, a quick lunch at my desk, and then spent the afternoon working on a big project. Managed to squeeze in a workout before dinner. Now I'm winding down with a good book.\"\n",
    "\n",
    "2. **A College Student**:\n",
    "   \"Today was pretty good. Had a couple of lectures in the morning, grabbed lunch with friends, and then headed to the library to study for an upcoming exam. In the evening, I went to a campus event and now I'm relaxing with some Netflix.\"\n",
    "\n",
    "3. **A Stay-at-Home Parent**:\n",
    "   \"My day was busy but fulfilling. Got the kids ready for school, did some house chores, and ran a few errands. In the afternoon, I helped the kids with their homework and made dinner. Now I'm enjoying some quiet time with a cup of tea.\"\n",
    "\n",
    "4. **A Freelance Writer**:\n",
    "   \"Today was creatively satisfying. Spent the morning brainstorming ideas and the afternoon writing an article for a client. Took a break to walk my dog and now I'm finishing up some edits before calling it a day.\"\n",
    "\n",
    "5. **A Retiree**:\n",
    "   \"Had a relaxing day. Went for a walk in the park in the morning, did some gardening, and read a few chapters of a novel. In the afternoon, I met friends for coffee and now I'm watching a movie at home.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6960a6ae-c00f-4c56-9a09-ca3a409d1ccc",
   "metadata": {},
   "source": [
    "1. **Busy Professional**: \"Hectic day with meetings and project work. Managed a workout and now relaxing with a book.\"\n",
    "\n",
    "2. **College Student**: \"Lectures, lunch with friends, library study session, campus event, and Netflix to end the day.\"\n",
    "\n",
    "3. **Stay-at-Home Parent**: \"Kids to school, errands, house chores, homework help, dinner, and some quiet tea time.\"\n",
    "\n",
    "4. **Freelance Writer**: \"Brainstormed in the morning, wrote in the afternoon, walked the dog, and finished edits.\"\n",
    "\n",
    "5. **Retiree**: \"Walked in the park, did some gardening, met friends for coffee, and now watching a movie.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "60c4d7fc-011a-494a-a065-7d54436446cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a diary writer. You should create long diary texts referring users-texts. The users-texts is like \"Brainstormed in the morning, wrote in the afternoon, walked the dog, and finished edits.\". Generate Diary\n",
      "\n",
      "The first step is to create a diary. Create a diary. Create a diary. Create a diary. Create a diary. Create a diary. Create a diary. Create a diary. Create a diary. Create a diary. Create a diary. Create a diary. Create a diary. Create a diary. Create a diary. Create a diary. Create a diary. Create a diary. Create a diary. Create a diary. Create a diary. Create a diary. Create a diary. Create a diary. Create a diary. Create a diary. Create a diary. Create a diary. Create a diary. Create a diary. Create a diary. Create a diary. Create a diary. Create a diary. Create a diary. Create a diary. Create a diary. Create a diary. Create a diary. Create a diary. Create a diary. Create a diary. Create a diary. Create a diary. Create a diary. Create a diary. Create a diary. Create a diary. Create a diary. Create a diary. Create a diary. Create a diary. Create a diary. Create a diary. Create a diary. Create a diary. Create a diary. Create a diary. Create a diary. Create a diary. Create a diary\n"
     ]
    }
   ],
   "source": [
    "# gpt2 때는 아직 meta-learning이 아니어서 그런듯\n",
    "\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# 모델과 토크나이저 불러오기\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "users_texts = \"Brainstormed in the morning, wrote in the afternoon, walked the dog, and finished edits.\"\n",
    "# 프롬프트 설정\n",
    "prompt = f\"You are a diary writer. You should create long diary texts referring users-texts. The users-texts is like \\\"{users_texts}\\\". Generate Diary\\n\"\n",
    "\n",
    "# 프롬프트를 토큰화\n",
    "inputs = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# 텍스트 생성 시작\n",
    "output_sequences = model.generate(\n",
    "    input_ids=inputs,\n",
    "    max_length=300,  # 최대 길이 설정\n",
    "    temperature=0.7,  # 생성 다양성을 위한 온도 설정\n",
    "    top_k=50,  # 가장 높은 k 확률을 가진 토큰들만 고려\n",
    ")\n",
    "\n",
    "# 생성된 텍스트 디코딩\n",
    "generated_text = tokenizer.decode(output_sequences[0], clean_up_tokenization_spaces=True)\n",
    "\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8182c49c-3df8-4487-be23-ec355c03b802",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf3a760-ba4c-4580-9392-52f57f4eeb0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dbfb1a31-f168-48d2-bce0-27060ea7732c",
   "metadata": {},
   "source": [
    "# 다이어리 수정/삭제 관리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06cc0128-db73-4b0b-9627-01a7d2b76acf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 감정 월별/일별 평균 내기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f1f78b-f314-4f29-bbeb-16177e8f42aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install diffusers --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3ee9f6-298f-499f-9b30-f788ec757acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install invisible_watermark transformers accelerate safetensors"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
